{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.1 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "f71034e9bda757e14c2db7fa332a6303fa23e192540cdeb9f2be353e6523bcac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/AsgerSturisTang/OneDrive - Danmarks Tekniske Universitet/DTU/6. Semester/Bachelor2021\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Modelling import modelling\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import plotly.express as px\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from DataPrep.ImportData import importer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = modelling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.03968904044452859"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "lm = m.lmmodels1()\n",
    "lm.score(m.X_test,m.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'CenterLon': -150681284258.90356,\n",
       " 'CenterLat': -1094.9506632871926,\n",
       " '# Professional & Other Places': -96120152048.2271,\n",
       " '# Food': 71960981277.76135,\n",
       " '# Shop & Service': 122394730938.68948,\n",
       " '# Travel & Transport': -70199952207.2378,\n",
       " '# Outdoors & Recreation': 23155922383.70183,\n",
       " '# Arts & Entertainment': 37311140499.7466,\n",
       " '# Nightlife Spot': -7449438767.08811,\n",
       " '# Residence': -169856267810.20935,\n",
       " '# College & University': 1497651300.3496356,\n",
       " '# Event': -57932083.4937479,\n",
       " 'Cluster_0': 1848543489.5147946,\n",
       " 'Cluster_1': 547503440.0513606,\n",
       " 'Cluster_2': -3091963920.2180834,\n",
       " 'Cluster_3': 91053992.1399763,\n",
       " 'Cluster_4': -45006317.86518467,\n",
       " 'Cluster_5': -488619819.05280715,\n",
       " 'Cluster_6': -69516913.88822378,\n",
       " 'Cluster_7': -53582338.37715954,\n",
       " 'Month_Day_1': 25986911.842409354,\n",
       " 'Month_Day_2': 25986911.907890256,\n",
       " 'Month_Day_3': 25986911.980078526,\n",
       " 'Month_Day_4': 25986911.84063305,\n",
       " 'Month_Day_5': 25986911.882016107,\n",
       " 'Month_Day_6': 25986912.030227333,\n",
       " 'Month_Day_7': 25986911.825545046,\n",
       " 'Month_Day_8': 25986911.79483846,\n",
       " 'Month_Day_9': 25986911.62247425,\n",
       " 'Month_Day_10': 25986911.802483812,\n",
       " 'Month_Day_11': 25986911.985422067,\n",
       " 'Month_Day_12': 25986911.937195543,\n",
       " 'Month_Day_13': 25986911.870265972,\n",
       " 'Month_Day_14': 25986911.812078,\n",
       " 'Month_Day_15': 25986911.723697662,\n",
       " 'Month_Day_16': 25986911.849758625,\n",
       " 'Month_Day_17': 25986911.875422478,\n",
       " 'Month_Day_18': 25986911.944404602,\n",
       " 'Month_Day_19': 25986911.701156616,\n",
       " 'Month_Day_20': 25986912.01933098,\n",
       " 'Month_Day_21': 25986911.881672144,\n",
       " 'Month_Day_22': 25986911.654987693,\n",
       " 'Month_Day_23': 25986911.729954243,\n",
       " 'Month_Day_24': 25986911.895312786,\n",
       " 'Month_Day_25': 25986912.002290726,\n",
       " 'Month_Day_26': 25986911.848024487,\n",
       " 'Month_Day_27': 25986911.77806282,\n",
       " 'Month_Day_28': 25986911.792547226,\n",
       " 'Month_Day_29': 25986911.84587443,\n",
       " 'Month_Day_30': 25986911.935645103,\n",
       " 'Month_Day_31': 25986911.91949463,\n",
       " 'Week_Day_0': 391779557.8775177,\n",
       " 'Week_Day_1': 391779557.6246338,\n",
       " 'Week_Day_2': 391779557.657074,\n",
       " 'Week_Day_3': 391779557.6618805,\n",
       " 'Week_Day_4': 391779557.52277756,\n",
       " 'Week_Day_5': 391779557.0920105,\n",
       " 'Week_Day_6': 391779557.43466187,\n",
       " 'Year_Month_1': 118854332.25460052,\n",
       " 'Year_Month_2': 118854332.26178169,\n",
       " 'Year_Month_3': 118854332.35807037,\n",
       " 'Year_Month_4': 118854332.31587219,\n",
       " 'Year_Month_5': 118854332.24031067,\n",
       " 'Year_Month_6': 118854332.05416393,\n",
       " 'Year_Month_7': 118854332.35827398,\n",
       " 'Year_Month_8': 118854332.22946167,\n",
       " 'Year_Month_9': 118854332.32133865,\n",
       " 'Year_Month_10': 118854332.25526428,\n",
       " 'Year_Month_11': 118854332.24060822,\n",
       " 'Year_Month_12': 118854332.31196594}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dict(zip(m.X_test.columns,lm.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - 2s 15ms/step - loss: 16.9914 - val_loss: 4.3541\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 7.1024 - val_loss: 4.3305\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 6.3173 - val_loss: 3.5706\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 5.5870 - val_loss: 3.2541\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 5.0997 - val_loss: 3.4807\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.8449 - val_loss: 2.8040\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.3418 - val_loss: 2.6951\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.4443 - val_loss: 2.7243\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 4.1869 - val_loss: 2.6714\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 4.1929 - val_loss: 2.6535\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.9926 - val_loss: 2.4620\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.9458 - val_loss: 2.4921\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.7393 - val_loss: 2.4724\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.6911 - val_loss: 2.6035\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.5278 - val_loss: 2.4591\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.4771 - val_loss: 2.4735\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.6615 - val_loss: 2.5223\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.3579 - val_loss: 2.4691\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2368 - val_loss: 2.4426\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2746 - val_loss: 2.4510\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2402 - val_loss: 2.4692\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.3109 - val_loss: 2.4692\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2033 - val_loss: 2.4528\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.2734 - val_loss: 2.4739\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.0847 - val_loss: 2.4114\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.1358 - val_loss: 2.4061\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.9884 - val_loss: 2.3996\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0995 - val_loss: 2.4347\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.0730 - val_loss: 2.4197\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.1290 - val_loss: 2.4076\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0268 - val_loss: 2.3972\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.9624 - val_loss: 2.3821\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.9812 - val_loss: 2.3858\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.8740 - val_loss: 2.3709\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8736 - val_loss: 2.3714\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.9164 - val_loss: 2.3979\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7702 - val_loss: 2.3591\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8472 - val_loss: 2.3537\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7937 - val_loss: 2.3593\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7961 - val_loss: 2.3805\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.8098 - val_loss: 2.3702\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.8335 - val_loss: 2.3666\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.8518 - val_loss: 2.3939\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7926 - val_loss: 2.3483\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6515 - val_loss: 2.3453\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7504 - val_loss: 2.3842\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6318 - val_loss: 2.3548\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6066 - val_loss: 2.3507\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7954 - val_loss: 2.3582\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7159 - val_loss: 2.3546\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7305 - val_loss: 2.3606\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6167 - val_loss: 2.3433\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7118 - val_loss: 2.3450\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6998 - val_loss: 2.3462\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 2.5977 - val_loss: 2.3504\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5961 - val_loss: 2.3390\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5429 - val_loss: 2.3339\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5262 - val_loss: 2.3293\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6162 - val_loss: 2.3398\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5999 - val_loss: 2.3484\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5716 - val_loss: 2.3455\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5917 - val_loss: 2.3292\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3696 - val_loss: 2.3461\n",
      "Epoch 64/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5086 - val_loss: 2.3302\n",
      "Epoch 65/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5056 - val_loss: 2.3377\n",
      "Epoch 66/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5279 - val_loss: 2.3322\n",
      "Epoch 67/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4330 - val_loss: 2.3306\n",
      "Epoch 68/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4606 - val_loss: 2.3395\n",
      "Epoch 69/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3947 - val_loss: 2.3263\n",
      "Epoch 70/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4267 - val_loss: 2.3375\n",
      "Epoch 71/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4266 - val_loss: 2.3519\n",
      "Epoch 72/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5256 - val_loss: 2.3293\n",
      "Epoch 73/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3894 - val_loss: 2.3353\n",
      "Epoch 74/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4172 - val_loss: 2.3239\n",
      "Epoch 75/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5143 - val_loss: 2.3259\n",
      "Epoch 76/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3873 - val_loss: 2.3245\n",
      "Epoch 77/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4281 - val_loss: 2.3494\n",
      "Epoch 78/100\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 2.5369 - val_loss: 2.3191\n",
      "Epoch 79/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4154 - val_loss: 2.3155\n",
      "Epoch 80/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5174 - val_loss: 2.3151\n",
      "Epoch 81/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4195 - val_loss: 2.3292\n",
      "Epoch 82/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3049 - val_loss: 2.3129\n",
      "Epoch 83/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3813 - val_loss: 2.3126\n",
      "Epoch 84/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2798 - val_loss: 2.3186\n",
      "Epoch 85/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3982 - val_loss: 2.3223\n",
      "Epoch 86/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4997 - val_loss: 2.3222\n",
      "Epoch 87/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3632 - val_loss: 2.3207\n",
      "Epoch 88/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2642 - val_loss: 2.3279\n",
      "Epoch 89/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3104 - val_loss: 2.3568\n",
      "Epoch 90/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3470 - val_loss: 2.3364\n",
      "Epoch 91/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3755 - val_loss: 2.3233\n",
      "Epoch 92/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4040 - val_loss: 2.3213\n",
      "Epoch 93/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3412 - val_loss: 2.3463\n",
      "Epoch 94/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3119 - val_loss: 2.3247\n",
      "Epoch 95/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3890 - val_loss: 2.3099\n",
      "Epoch 96/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3473 - val_loss: 2.3136\n",
      "Epoch 97/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2956 - val_loss: 2.3255\n",
      "Epoch 98/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2997 - val_loss: 2.3135\n",
      "Epoch 99/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3453 - val_loss: 2.3146\n",
      "Epoch 100/100\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2371 - val_loss: 2.3182\n",
      "\n",
      "MAE=1.078283\n",
      "r^2=0.433142\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4331424849493408"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "m.neuralnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "98/98 [==============================] - 1s 6ms/step - loss: 20.0049 - val_loss: 4.3715\n",
      "Epoch 2/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.9641 - val_loss: 3.9412\n",
      "Epoch 3/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.6621 - val_loss: 3.9303\n",
      "Epoch 4/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.5636 - val_loss: 3.9155\n",
      "Epoch 5/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.3891 - val_loss: 3.8769\n",
      "Epoch 6/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.5414 - val_loss: 3.8111\n",
      "Epoch 7/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.2044 - val_loss: 3.7001\n",
      "Epoch 8/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 5.0840 - val_loss: 3.5200\n",
      "Epoch 9/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.9853 - val_loss: 3.2778\n",
      "Epoch 10/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.6347 - val_loss: 3.0578\n",
      "Epoch 11/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.2436 - val_loss: 2.8758\n",
      "Epoch 12/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.9603 - val_loss: 2.7652\n",
      "Epoch 13/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 4.1537 - val_loss: 2.6783\n",
      "Epoch 14/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.9155 - val_loss: 2.6068\n",
      "Epoch 15/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.7046 - val_loss: 2.5664\n",
      "Epoch 16/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.7168 - val_loss: 2.5298\n",
      "Epoch 17/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.6487 - val_loss: 2.4948\n",
      "Epoch 18/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.5306 - val_loss: 2.4656\n",
      "Epoch 19/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.4333 - val_loss: 2.4582\n",
      "Epoch 20/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.5050 - val_loss: 2.4450\n",
      "Epoch 21/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2951 - val_loss: 2.4344\n",
      "Epoch 22/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.4245 - val_loss: 2.4336\n",
      "Epoch 23/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.3342 - val_loss: 2.4288\n",
      "Epoch 24/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.4747 - val_loss: 2.4302\n",
      "Epoch 25/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.2325 - val_loss: 2.4157\n",
      "Epoch 26/150\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 3.3035 - val_loss: 2.4084\n",
      "Epoch 27/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 3.1977 - val_loss: 2.4158\n",
      "Epoch 28/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2695 - val_loss: 2.4146\n",
      "Epoch 29/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.1751 - val_loss: 2.4025\n",
      "Epoch 30/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2055 - val_loss: 2.3997\n",
      "Epoch 31/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.2202 - val_loss: 2.3984\n",
      "Epoch 32/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.1036 - val_loss: 2.3970\n",
      "Epoch 33/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0044 - val_loss: 2.3941\n",
      "Epoch 34/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0941 - val_loss: 2.3883\n",
      "Epoch 35/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0946 - val_loss: 2.4046\n",
      "Epoch 36/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0541 - val_loss: 2.3867\n",
      "Epoch 37/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0039 - val_loss: 2.3820\n",
      "Epoch 38/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.9931 - val_loss: 2.3823\n",
      "Epoch 39/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 3.0121 - val_loss: 2.3746\n",
      "Epoch 40/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.9695 - val_loss: 2.3868\n",
      "Epoch 41/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8673 - val_loss: 2.3871\n",
      "Epoch 42/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.9692 - val_loss: 2.3782\n",
      "Epoch 43/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7576 - val_loss: 2.3868\n",
      "Epoch 44/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.9955 - val_loss: 2.3744\n",
      "Epoch 45/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8701 - val_loss: 2.3740\n",
      "Epoch 46/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7674 - val_loss: 2.3780\n",
      "Epoch 47/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8511 - val_loss: 2.3745\n",
      "Epoch 48/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8496 - val_loss: 2.3756\n",
      "Epoch 49/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7888 - val_loss: 2.3720\n",
      "Epoch 50/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8504 - val_loss: 2.3733\n",
      "Epoch 51/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7292 - val_loss: 2.3663\n",
      "Epoch 52/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6592 - val_loss: 2.3665\n",
      "Epoch 53/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7471 - val_loss: 2.3632\n",
      "Epoch 54/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.8419 - val_loss: 2.3722\n",
      "Epoch 55/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7272 - val_loss: 2.3657\n",
      "Epoch 56/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7333 - val_loss: 2.3586\n",
      "Epoch 57/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6394 - val_loss: 2.3586\n",
      "Epoch 58/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6293 - val_loss: 2.3576\n",
      "Epoch 59/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7075 - val_loss: 2.3572\n",
      "Epoch 60/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6830 - val_loss: 2.3576\n",
      "Epoch 61/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7630 - val_loss: 2.3561\n",
      "Epoch 62/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5994 - val_loss: 2.3542\n",
      "Epoch 63/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5423 - val_loss: 2.3605\n",
      "Epoch 64/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.8200 - val_loss: 2.3556\n",
      "Epoch 65/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.7151 - val_loss: 2.3519\n",
      "Epoch 66/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6425 - val_loss: 2.3534\n",
      "Epoch 67/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.6780 - val_loss: 2.3554\n",
      "Epoch 68/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5006 - val_loss: 2.3544\n",
      "Epoch 69/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5357 - val_loss: 2.3525\n",
      "Epoch 70/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5303 - val_loss: 2.3504\n",
      "Epoch 71/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5488 - val_loss: 2.3536\n",
      "Epoch 72/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5521 - val_loss: 2.3537\n",
      "Epoch 73/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5150 - val_loss: 2.3512\n",
      "Epoch 74/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.7163 - val_loss: 2.3488\n",
      "Epoch 75/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.6013 - val_loss: 2.3461\n",
      "Epoch 76/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5682 - val_loss: 2.3443\n",
      "Epoch 77/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5737 - val_loss: 2.3477\n",
      "Epoch 78/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4949 - val_loss: 2.3478\n",
      "Epoch 79/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5495 - val_loss: 2.3458\n",
      "Epoch 80/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5386 - val_loss: 2.3470\n",
      "Epoch 81/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4702 - val_loss: 2.3442\n",
      "Epoch 82/150\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 2.5142 - val_loss: 2.3428\n",
      "Epoch 83/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4063 - val_loss: 2.3487\n",
      "Epoch 84/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5402 - val_loss: 2.3443\n",
      "Epoch 85/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5003 - val_loss: 2.3437\n",
      "Epoch 86/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4698 - val_loss: 2.3456\n",
      "Epoch 87/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5072 - val_loss: 2.3499\n",
      "Epoch 88/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4219 - val_loss: 2.3450\n",
      "Epoch 89/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5299 - val_loss: 2.3535\n",
      "Epoch 90/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5543 - val_loss: 2.3476\n",
      "Epoch 91/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4688 - val_loss: 2.3447\n",
      "Epoch 92/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4172 - val_loss: 2.3431\n",
      "Epoch 93/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4524 - val_loss: 2.3426\n",
      "Epoch 94/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5055 - val_loss: 2.3461\n",
      "Epoch 95/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3640 - val_loss: 2.3427\n",
      "Epoch 96/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4595 - val_loss: 2.3434\n",
      "Epoch 97/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5257 - val_loss: 2.3425\n",
      "Epoch 98/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4783 - val_loss: 2.3433\n",
      "Epoch 99/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5302 - val_loss: 2.3430\n",
      "Epoch 100/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.5199 - val_loss: 2.3473\n",
      "Epoch 101/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4087 - val_loss: 2.3461\n",
      "Epoch 102/150\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.4041 - val_loss: 2.3472\n",
      "Epoch 103/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4430 - val_loss: 2.3479\n",
      "Epoch 104/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4642 - val_loss: 2.3439\n",
      "Epoch 105/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4529 - val_loss: 2.3423\n",
      "Epoch 106/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4488 - val_loss: 2.3422\n",
      "Epoch 107/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3446 - val_loss: 2.3406\n",
      "Epoch 108/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4528 - val_loss: 2.3424\n",
      "Epoch 109/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4421 - val_loss: 2.3457\n",
      "Epoch 110/150\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 2.4873 - val_loss: 2.3449\n",
      "Epoch 111/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4266 - val_loss: 2.3503\n",
      "Epoch 112/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4448 - val_loss: 2.3425\n",
      "Epoch 113/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3588 - val_loss: 2.3540\n",
      "Epoch 114/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4055 - val_loss: 2.3407\n",
      "Epoch 115/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4719 - val_loss: 2.3454\n",
      "Epoch 116/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4569 - val_loss: 2.3402\n",
      "Epoch 117/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4138 - val_loss: 2.3485\n",
      "Epoch 118/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3769 - val_loss: 2.3419\n",
      "Epoch 119/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4782 - val_loss: 2.3419\n",
      "Epoch 120/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3486 - val_loss: 2.3414\n",
      "Epoch 121/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4193 - val_loss: 2.3423\n",
      "Epoch 122/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5072 - val_loss: 2.3377\n",
      "Epoch 123/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3374 - val_loss: 2.3395\n",
      "Epoch 124/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4932 - val_loss: 2.3393\n",
      "Epoch 125/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3648 - val_loss: 2.3448\n",
      "Epoch 126/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3682 - val_loss: 2.3416\n",
      "Epoch 127/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3734 - val_loss: 2.3408\n",
      "Epoch 128/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.5444 - val_loss: 2.3457\n",
      "Epoch 129/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3624 - val_loss: 2.3366\n",
      "Epoch 130/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3621 - val_loss: 2.3331\n",
      "Epoch 131/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3399 - val_loss: 2.3349\n",
      "Epoch 132/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3648 - val_loss: 2.3392\n",
      "Epoch 133/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3526 - val_loss: 2.3336\n",
      "Epoch 134/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4069 - val_loss: 2.3313\n",
      "Epoch 135/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3876 - val_loss: 2.3353\n",
      "Epoch 136/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3428 - val_loss: 2.3327\n",
      "Epoch 137/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3805 - val_loss: 2.3334\n",
      "Epoch 138/150\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 2.3762 - val_loss: 2.3362\n",
      "Epoch 139/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3434 - val_loss: 2.3420\n",
      "Epoch 140/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3287 - val_loss: 2.3332\n",
      "Epoch 141/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3947 - val_loss: 2.3306\n",
      "Epoch 142/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2937 - val_loss: 2.3397\n",
      "Epoch 143/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3287 - val_loss: 2.3379\n",
      "Epoch 144/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3265 - val_loss: 2.3349\n",
      "Epoch 145/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4068 - val_loss: 2.3346\n",
      "Epoch 146/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3425 - val_loss: 2.3280\n",
      "Epoch 147/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3646 - val_loss: 2.3308\n",
      "Epoch 148/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.4346 - val_loss: 2.3294\n",
      "Epoch 149/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.2905 - val_loss: 2.3274\n",
      "Epoch 150/150\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 2.3522 - val_loss: 2.3280\n",
      "\n",
      "MAE=1.075483\n",
      "r^2=0.424572\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=63, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(40, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(m.X_train,m.y_train, epochs = 150, batch_size=128, validation_data=(m.X_val,m.y_val))\n",
    "\n",
    "# evaluate the keras model\n",
    "y_pred = model.predict(m.X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "print(\"\\nMAE=%f\" % mean_absolute_error(m.y_test, y_pred))\n",
    "print(\"r^2=%f\" % r2_score(m.y_test, y_pred))"
   ]
  }
 ]
}